Tensor Network State
####################
In differential geometry, the tensor at a point is based on the tangent space at 
that point on the manifold. A (m, n) type tensor refers to a multi-linear map 
that maps m covariant vectors (cotangent vectors) and n inverter vectors (tangent 
vectors) to the number domain. Given a set of bases, this multi-linear mapping 
can be represented by a series of components. These components are of course 
related to the choice of basis. In a tensor network, we usually don't do 
coordinate transformation, so any component set with n indicators is called a 
tensor. 

\begin{figure}[H]
\centering
\includegraphics{./images/tensor_notation}
\caption{Tensor diagrams: (a) scalar, (b) vector, (c) matrix  (d) rank-3 tensor}
\end{figure}

Quantum mechanics has a basic assumption: Hilbert space of a composite system 
is the tensor product of its subsystem Hilbert space. Therefore, the multi-body 
wave function is naturally a tensor,
for an n-body system, its wave function is an n--th order tensor. Of course, 
since the linear space with the same dimension is isomorphic, the wave function
can also be regarded as a vector, and the dimension of this vector is $d^n$. 
The Hamiltonian of the n-body system is a $d^n$-dimensional matrix, which can of 
course also be regarded as a (n, n) tensor. The single operator can be viewed as 
a second-order tensor. The two-body operator can be viewed as a second-order 
tensor with $d^2$ dimensions, or as a (2,2) type (fourth-order) tensor with 
dimension d. The average value of all observable measurements in quantum 
mechanics can be written as an inner product, which is quadratic from the matrix 
point of view. If the wave function is in the form of a tensor, the operator can 
also be written as the sum of local tensors, then the average value of the 
operator can also be regarded as the sum of the contractions of these tensors.
\begin{table}[ht]
\centering
% To place a caption above a table
\begin{tabular}{l|c|l|c}
  \hline
  Type&	 Rank &	 Example &	Notation\\
  \hline
  \hline
  Scalar&	 0&	Constants & $\lambda$ \\
  \hline
  Vector& 1& Wave function &$\psi_{i}$\\
  \hline
  Matrix& 2 &Operator & $O_{i,j}$\\
  \hline
  $\vdots$ & $\vdots$	 & $\vdots$ & $\vdots$ \\
  \hline
  Rank--N tensor& N & N--body wave function & $\Psi_{\alpha_1,\dots,\alpha_N}$\\
  \hline
\end{tabular}
% Or to place a caption below a table
\caption{Tensors of different rank-N.}
\label{tab:tensor_type}
\end{table}%

Tensor network states ansatz is used to solve chemistry system with a strong
correlation. Two optimization methods base on tensor network states are
implemented in MoHa, which include variational optimization and time-evolving
block decimation method.

Time-evolving block decimation (TEBD)
======================================
* Consider a Hamiltonian of the form
.. math::
    H = \sum_j h^{[j,j+1]}

* Decompose the Hamiltonian as $H=F+G$
.. math::
    F = \sum_{even} F^{[j]} = \sum_{even j}h^{[j,j+1]}
    G = \sum_{odd} G^{[j]} = \sum_{odd j}h^{[j,j+1]}

* Apply Suzuki-Trotter decomposition, and two chanins of two-site gets
.. math::
    U_{F} = \prod_{even r}exp(-iF^{[r]}\delta t)
    U_{G} = \prod_{odd r}exp(-iG^{[r]}\delta t)

.. figure:: ./pictures/tebd.png
        :scale: 50 %
        :align: center

.. literalinclude:: ../data/examples/tns/tebd_heisenberg.py
        :lines: 1-20
        :caption: /data/examples/tns/tebd_heisenberg.py

Variational Optimization
========================
* Variation principle
.. math::
    \frac{\langle\Psi |H| \Psi\rangle}{\langle\Psi|\Psi\rangle}\geq E_0

* Approach the ground state energy by minimizing
.. math::
    min_{|\Psi\rangle}(\langle\Psi |H| \Psi\rangle - \lambda \langle\Psi|\Psi\rangle)
* Minimize one tensor each time, keeping the other fixed
* sweep over all the tensor several times

.. literalinclude:: ../data/examples/tns/dmrg_heisenberg.py
        :lines: 1-20
        :caption: /data/examples/tns/dmrg_heisenberg.py

Projected Optimization
======================

We can have a trial for solving tensor network state problem by projected approach. 
For simplisity, we start with MPS which is

.. math::
    |MPS\rangle
    =\sum_{\textbf{n}}Tr(U^{\alpha_1}_{m_1}U^{\alpha_2}_{m_1m_2}U^{\alpha_3}_{m_2m_3}\dots)
    |\textbf{n}\rangle

To slove $\hat{H}|MPS\rangle = E|MPS\rangle$, We may set up N equantions to
find out the energy and amplitudes by left projecting a known simple states
$\{\Phi_{\alpha}\}$i,

.. math::
    \langle HF|\hat{H}|MPS\rangle = E\langle HF|MPS\rangle

.. math::
    \langle \Phi_1|\hat{H}|MPS\rangle = E\langle \Phi_1|MPS\rangle

.. math::
    \dots

.. math::
    \langle \Phi_N|\hat{H}|MPS\rangle = E\langle \Phi_N|MPS\rangle

To get the approximate amplitudes, a set of nonlinear equation should be sloved

.. math::
    f_{\alpha}(\textbf{U}) := \langle
    \Phi_{\alpha}|\hat{H}-E|MPS(\textbf{U})\rangle  = 0

Which is usually sloved by quasi-Newton methods,

.. math::
    \textbf{U}^{(\alpha+1)} = \textbf{U}^{(\alpha)} -
    \textbf{F}^{-1}f(\textbf{U}^{(\alpha)}) \ \ \textbf{U}^{0} = 0

The last step of projected MPS is by solving these equantions self consistently, yields approximate amplitudes and an approximate energy. 

TODO
